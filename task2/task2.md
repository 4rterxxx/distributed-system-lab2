# Задание 2: Моделирование задачи N тел

## 1. Постановка задачи

**Цель:** Реализовать параллельную программу для моделирования гравитационного взаимодействия N материальных точек (задача N тел) с использованием метода Эйлера первого порядка и технологии OpenMP.

**Физическая модель:** Система из N частиц взаимодействует по закону всемирного тяготения Ньютона. Сила, действующая на частицу i со стороны частицы j:
F_ij = G * (m_i * m_j) / (r_ij³) * (r_j - r_i)
где G = 6.67430e-11 — гравитационная постоянная, r_ij — расстояние между частицами.

**Численный метод:** Метод Эйлера первого порядка с шагом Δt = 3600 секунд (1 час):
v_i_new = v_i_old + a_i * Δt
r_i_new = r_i_old + v_i_old * Δt
где a_i = сумма всех сил, действующих на частицу i, деленная на ее массу.

**Аппаратная архитектура для OpenMP:**
- Процессор: 4 процессора × 2 ядра = 8 физических ядер
- Операционная система: Debian Linux
- Компилятор: gcc с поддержкой OpenMP

**Аппаратная архитектура для CUDA:**
- GPU: NVIDIA Tesla T4, 15360 MiB памяти
- Google Collab
- Компилятор: nvcc для CUDA

**Методика измерений:**
- OpenMP: время измерялось однократно, полное время выполнения программы с помощью omp_get_wtime()
- CUDA: время измерялось с помощью CUDA Events (cudaEventElapsedTime), учитывая только вычислительную часть
- Использовалась функция `omp_get_wtime()` для точного замера
- Для каждого сочетания параметров проводился отдельный запуск
- Шаг по времени Δt = 3600 секунд

## 2. Реализация алгоритма

### Структура данных
```c
typedef struct {
    double mass, x, y, z, vx, vy, vz, ax, ay, az;
} Particle;
```

### Основные этапы алгоритма:
1. Чтение входных данных: массы, начальные координаты и скорости частиц
2. Вычисление ускорений: O(N²) операций, распараллеливается по внешнему циклу
3. Интегрирование уравнений движения: обновление скоростей и координат
4. Запись траекторий: вывод в CSV-формат


### Параллельная реализация на OpenMP


**1. Установка числа потоков:**
```c
omp_set_num_threads(num_threads);
```

**2. Распараллеливание вычисления ускорений:**
```c
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // вычисление суммарного ускорения для частицы i
    // от всех остальных частиц j
}
```

**3. Распараллеливание обновления состояний:**
```c
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    // обновление скорости и координат частицы i
}
```

### Особенности реализации
- Разделение вычислений: два независимых параллельных цикла (ускорения и обновление)
- Избегание гонок данных: каждый поток вычисляет ускорения для своего набора частиц
- Локальность данных: структура Particle хранит все параметры частицы вместе

### CUDA реализация 

**CUDA ядра:**
```cuda
__global__ void compute_accelerations(Particle* p, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= n) return;
    
    // каждая нить вычисляет ускорения для одной частицы
    // полный цикл по всем остальным частицам
}

__global__ void update_particles(Particle* p, float dt, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i >= n) return;
    
    // обновление скоростей и координат
}
```

## 3. Результаты экспериментов

### Время выполнения OpenMP(секунды)

| Количество частиц | 1 поток | 2 потока | 4 потока | 8 потоков |
|------------------|---------|----------|----------|-----------|
| 50               | 0.000495| 0.000436 | 0.000591 | 0.000998  |
| 500              | 0.009446| 0.006661 | 0.005808 | 0.005655  |
| 5000             | 0.755863| 0.409038 | 0.229413 | 0.202311  |

### CUDA результаты

| Количество частиц | Размер блока | Время (мс) | Время на шаг (мс) |
|-------------------|--------------|------------|-------------------|
| 50                | 32           | 0.323296   | 0.032330          |
| 50                | 64           | 0.309440   | 0.030944          |
| 50                | 128          | 0.324864   | 0.032486          |
| 50                | 256          | 0.284704   | 0.028470          |
|                   |              |            |                   |
| 500               | 32           | 1.404224   | 0.140422          |
| 500               | 64           | 1.445888   | 0.144589          |
| 500               | 128          | 1.459424   | 0.145942          |
| 500               | 256          | 1.409056   | 0.140906          |
|                   |              |            |                   |
| 5000              | 32           | 13.145632  | 1.314563          |
| 5000              | 64           | 13.170784  | 1.317078          |
| 5000              | 128          | 13.226752  | 1.322675          |
| 5000              | 256          | 13.181120  | 1.318112          |

## Сравнительный анализ OpenMP vs CUDA

###  Абсолютное время выполнения (лучшие результаты)

| Количество частиц | OpenMP (мс) | CUDA (мс) | Во сколько раз CUDA быстрее |
|-------------------|-------------|-----------|----------------------------|
| 50                | 0.436       | 0.285     | ~1.53x (CUDA быстрее) |
| 500               | 5.655       | 1.404     | ~4.03x (CUDA быстрее) |
| 5000              | 202.311     | 13.146    | ~15.4x (CUDA быстрее) |


## 4. Анализ результатов

#### Для малых систем (50 частиц):
- **OpenMP:** 0.436 мс (2 потока)
- **CUDA:** 0.285 мс (256 размер блока)
- **Вывод:** CUDA показывает сопоставимую производительность, но с учетом передачи данных OpenMP может быть быстрее

#### Для средних систем (500 частиц):
- **OpenMP:** 5.655 мс (8 потоков)
- **CUDA:** 1.404 мс (32 размер блока)
- **Вывод:** CUDA в ~4 раза быстрее в чистом вычислении

#### Для больших систем (5000 частиц):
- **OpenMP:** 202.311 мс (8 потоков)
- **CUDA:** 13.146 мс (32 размер блока)
- **Вывод:** CUDA в ~15.4 раза быстрее в чистом вычислении

## 5. Вывод

В ходе работы реализованы две параллельные версии моделирования задачи N тел: на CPU с OpenMP и на GPU с CUDA. GPU демонстрирует значительное ускорение для больших систем (до 15 раз для 5000 частиц), но для малых задач CPU эффективнее из-за накладных расходов на передачу данных. Оптимальный размер блока CUDA для Tesla T4 составил 256. Основным ограничением для обеих реализаций остается алгоритмическая сложность O(N²). Для практического применения рекомендуется использовать CPU для систем менее 100 частиц, а GPU — для более крупных задач.




